{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: Book Crossing Dataset\n",
    "\n",
    "This notebook provides a comprehensive EDA of the Book Crossing dataset, focusing on:\n",
    "1. Dataset overview and basic statistics\n",
    "2. Interaction sparsity analysis\n",
    "3. User and item activity distributions\n",
    "4. Temporal dynamics\n",
    "5. Data pathologies (popularity skew, cold-start)\n",
    "6. Modeling implications and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "from src.data.loader import load_all_data, load_ratings, load_books, load_users\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "ratings, books, users = load_all_data('../data/raw')\n",
    "\n",
    "print(\"Dataset Sizes:\")\n",
    "print(f\"  Ratings: {len(ratings):,} interactions\")\n",
    "print(f\"  Books: {len(books):,} items\")\n",
    "print(f\"  Users: {len(users):,} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine ratings structure\n",
    "print(\"Ratings DataFrame:\")\n",
    "print(ratings.head(10))\n",
    "print(f\"\\nRatings dtypes:\\n{ratings.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine books structure\n",
    "print(\"Books DataFrame:\")\n",
    "print(books.head())\n",
    "print(f\"\\nBooks dtypes:\\n{books.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine users structure\n",
    "print(\"Users DataFrame:\")\n",
    "print(users.head())\n",
    "print(f\"\\nUsers dtypes:\\n{users.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Rating Value Distribution\n",
    "\n",
    "The Book Crossing dataset has a unique rating scheme:\n",
    "- **0**: Implicit feedback (user interacted with the book but didn't rate)\n",
    "- **1-10**: Explicit ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating value distribution\n",
    "rating_counts = ratings['rating'].value_counts().sort_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# All ratings\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(rating_counts.index, rating_counts.values, color='steelblue', edgecolor='black')\n",
    "bars[0].set_color('salmon')  # Highlight implicit ratings\n",
    "ax1.set_xlabel('Rating Value')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Distribution of All Ratings')\n",
    "ax1.set_xticks(range(11))\n",
    "\n",
    "# Add count labels\n",
    "for bar, count in zip(bars, rating_counts.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "             f'{count:,}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Explicit ratings only (1-10)\n",
    "explicit_ratings = ratings[ratings['rating'] > 0]['rating']\n",
    "ax2 = axes[1]\n",
    "explicit_counts = explicit_ratings.value_counts().sort_index()\n",
    "ax2.bar(explicit_counts.index, explicit_counts.values, color='steelblue', edgecolor='black')\n",
    "ax2.set_xlabel('Rating Value')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Distribution of Explicit Ratings (1-10)')\n",
    "ax2.set_xticks(range(1, 11))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/eda_rating_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "implicit_count = (ratings['rating'] == 0).sum()\n",
    "explicit_count = (ratings['rating'] > 0).sum()\n",
    "print(f\"\\nImplicit ratings (0): {implicit_count:,} ({100*implicit_count/len(ratings):.1f}%)\")\n",
    "print(f\"Explicit ratings (1-10): {explicit_count:,} ({100*explicit_count/len(ratings):.1f}%)\")\n",
    "print(f\"\\nExplicit ratings statistics:\")\n",
    "print(explicit_ratings.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interaction Sparsity Analysis\n",
    "\n",
    "Sparsity is a critical metric for recommender systems. High sparsity challenges collaborative filtering methods and informs the choice of algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sparsity metrics\n",
    "n_users = ratings['user_id'].nunique()\n",
    "n_items = ratings['isbn'].nunique()\n",
    "n_interactions = len(ratings)\n",
    "\n",
    "# Total possible interactions\n",
    "total_possible = n_users * n_items\n",
    "\n",
    "# Sparsity\n",
    "density = n_interactions / total_possible\n",
    "sparsity = 1 - density\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INTERACTION SPARSITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nUnique users in ratings: {n_users:,}\")\n",
    "print(f\"Unique items in ratings: {n_items:,}\")\n",
    "print(f\"Total interactions: {n_interactions:,}\")\n",
    "print(f\"\\nTotal possible interactions: {total_possible:,}\")\n",
    "print(f\"Density: {density:.6f} ({density*100:.4f}%)\")\n",
    "print(f\"Sparsity: {sparsity:.6f} ({sparsity*100:.4f}%)\")\n",
    "print(f\"\\nAverage interactions per user: {n_interactions/n_users:.2f}\")\n",
    "print(f\"Average interactions per item: {n_interactions/n_items:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity for explicit ratings only\n",
    "explicit_df = ratings[ratings['rating'] > 0]\n",
    "n_users_explicit = explicit_df['user_id'].nunique()\n",
    "n_items_explicit = explicit_df['isbn'].nunique()\n",
    "n_explicit = len(explicit_df)\n",
    "\n",
    "total_possible_explicit = n_users_explicit * n_items_explicit\n",
    "density_explicit = n_explicit / total_possible_explicit\n",
    "sparsity_explicit = 1 - density_explicit\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPLICIT RATINGS ONLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nUnique users: {n_users_explicit:,}\")\n",
    "print(f\"Unique items: {n_items_explicit:,}\")\n",
    "print(f\"Total explicit ratings: {n_explicit:,}\")\n",
    "print(f\"\\nDensity: {density_explicit:.6f} ({density_explicit*100:.4f}%)\")\n",
    "print(f\"Sparsity: {sparsity_explicit:.6f} ({sparsity_explicit*100:.4f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sparsity comparison with other datasets\n",
    "datasets_sparsity = {\n",
    "    'Book Crossing\\n(all)': sparsity * 100,\n",
    "    'Book Crossing\\n(explicit)': sparsity_explicit * 100,\n",
    "    'MovieLens 100K\\n(reference)': 93.7,\n",
    "    'MovieLens 1M\\n(reference)': 95.8,\n",
    "    'Netflix Prize\\n(reference)': 98.8\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(datasets_sparsity.keys(), datasets_sparsity.values(), \n",
    "              color=['#e74c3c', '#e74c3c', '#3498db', '#3498db', '#3498db'],\n",
    "              edgecolor='black')\n",
    "ax.set_ylabel('Sparsity (%)')\n",
    "ax.set_title('Sparsity Comparison: Book Crossing vs. Other Datasets')\n",
    "ax.set_ylim(90, 100)\n",
    "\n",
    "for bar, val in zip(bars, datasets_sparsity.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "            f'{val:.2f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/eda_sparsity_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. User and Item Activity Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate user and item activity\n",
    "user_activity = ratings.groupby('user_id').size()\n",
    "item_activity = ratings.groupby('isbn').size()\n",
    "\n",
    "print(\"USER ACTIVITY STATISTICS\")\n",
    "print(user_activity.describe())\n",
    "print(f\"\\nUsers with 1 rating: {(user_activity == 1).sum():,} ({100*(user_activity == 1).sum()/len(user_activity):.1f}%)\")\n",
    "print(f\"Users with ≤5 ratings: {(user_activity <= 5).sum():,} ({100*(user_activity <= 5).sum()/len(user_activity):.1f}%)\")\n",
    "print(f\"Users with ≤10 ratings: {(user_activity <= 10).sum():,} ({100*(user_activity <= 10).sum()/len(user_activity):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nITEM ACTIVITY STATISTICS\")\n",
    "print(item_activity.describe())\n",
    "print(f\"\\nItems with 1 rating: {(item_activity == 1).sum():,} ({100*(item_activity == 1).sum()/len(item_activity):.1f}%)\")\n",
    "print(f\"Items with ≤5 ratings: {(item_activity <= 5).sum():,} ({100*(item_activity <= 5).sum()/len(item_activity):.1f}%)\")\n",
    "print(f\"Items with ≤10 ratings: {(item_activity <= 10).sum():,} ({100*(item_activity <= 10).sum()/len(item_activity):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Distribution plots - using log scale to handle extreme skew\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# User activity histogram with log y-scale\nax1 = axes[0, 0]\n# Use logarithmic bins for better visualization of long-tail\nmax_val = int(user_activity.quantile(0.99))\nbins = np.logspace(0, np.log10(max_val + 1), 50)\nax1.hist(user_activity[user_activity <= max_val], bins=bins, color='steelblue', edgecolor='black', alpha=0.7)\nax1.set_xscale('log')\nax1.set_yscale('log')\nax1.set_xlabel('Number of Ratings (log scale)')\nax1.set_ylabel('Number of Users (log scale)')\nax1.set_title('User Activity Distribution')\nax1.axvline(user_activity.median(), color='red', linestyle='--', linewidth=2, label=f'Median: {user_activity.median():.0f}')\nax1.axvline(user_activity.mean(), color='orange', linestyle='--', linewidth=2, label=f'Mean: {user_activity.mean():.1f}')\nax1.legend()\n\n# User activity - cumulative distribution\nax2 = axes[0, 1]\nsorted_activity = np.sort(user_activity)\ncumulative = np.arange(1, len(sorted_activity) + 1) / len(sorted_activity)\nax2.plot(sorted_activity, 1 - cumulative, 'b-', linewidth=2)  # CCDF\nax2.set_xscale('log')\nax2.set_yscale('log')\nax2.set_xlabel('Number of Ratings (log scale)')\nax2.set_ylabel('P(X > x) - Fraction of users (log scale)')\nax2.set_title('User Activity CCDF (Complementary CDF)')\nax2.axvline(5, color='red', linestyle=':', alpha=0.7, label='Threshold: 5 ratings')\nax2.axvline(10, color='orange', linestyle=':', alpha=0.7, label='Threshold: 10 ratings')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Item activity histogram with log y-scale\nax3 = axes[1, 0]\nmax_val_item = int(item_activity.quantile(0.99))\nbins_item = np.logspace(0, np.log10(max_val_item + 1), 50)\nax3.hist(item_activity[item_activity <= max_val_item], bins=bins_item, color='coral', edgecolor='black', alpha=0.7)\nax3.set_xscale('log')\nax3.set_yscale('log')\nax3.set_xlabel('Number of Ratings (log scale)')\nax3.set_ylabel('Number of Items (log scale)')\nax3.set_title('Item (Book) Activity Distribution')\nax3.axvline(item_activity.median(), color='red', linestyle='--', linewidth=2, label=f'Median: {item_activity.median():.0f}')\nax3.axvline(item_activity.mean(), color='orange', linestyle='--', linewidth=2, label=f'Mean: {item_activity.mean():.1f}')\nax3.legend()\n\n# Item activity - cumulative distribution\nax4 = axes[1, 1]\nsorted_item_activity = np.sort(item_activity)\ncumulative_item = np.arange(1, len(sorted_item_activity) + 1) / len(sorted_item_activity)\nax4.plot(sorted_item_activity, 1 - cumulative_item, 'r-', linewidth=2)  # CCDF\nax4.set_xscale('log')\nax4.set_yscale('log')\nax4.set_xlabel('Number of Ratings (log scale)')\nax4.set_ylabel('P(X > x) - Fraction of items (log scale)')\nax4.set_title('Item Activity CCDF (Complementary CDF)')\nax4.axvline(5, color='red', linestyle=':', alpha=0.7, label='Threshold: 5 ratings')\nax4.axvline(10, color='orange', linestyle=':', alpha=0.7, label='Threshold: 10 ratings')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('../experiments/eda_activity_distributions.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Print interpretation\nprint(\"\\nINTERPRETATION:\")\nprint(\"-\" * 50)\nprint(f\"User activity: Median={user_activity.median():.0f}, Mean={user_activity.mean():.1f}\")\nprint(f\"  → Most users rate only {user_activity.median():.0f} book(s), but some rate hundreds\")\nprint(f\"  → The mean ({user_activity.mean():.1f}) >> median ({user_activity.median():.0f}) confirms heavy right skew\")\nprint(f\"\\nItem activity: Median={item_activity.median():.0f}, Mean={item_activity.mean():.1f}\")\nprint(f\"  → Most books have only {item_activity.median():.0f} rating(s)\")\nprint(f\"  → A few popular books dominate with thousands of ratings\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative distribution - Lorenz curve for activity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# User Lorenz curve\n",
    "ax1 = axes[0]\n",
    "user_sorted = np.sort(user_activity.values)\n",
    "user_cum = np.cumsum(user_sorted) / user_sorted.sum()\n",
    "user_pct = np.arange(1, len(user_sorted) + 1) / len(user_sorted)\n",
    "ax1.plot(user_pct, user_cum, 'b-', linewidth=2, label='Actual')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Perfect equality')\n",
    "ax1.fill_between(user_pct, user_cum, user_pct, alpha=0.2)\n",
    "ax1.set_xlabel('Cumulative % of Users')\n",
    "ax1.set_ylabel('Cumulative % of Ratings')\n",
    "ax1.set_title('User Activity Lorenz Curve')\n",
    "ax1.legend()\n",
    "\n",
    "# Calculate Gini coefficient for users\n",
    "n = len(user_sorted)\n",
    "gini_user = (2 * np.sum(np.arange(1, n+1) * user_sorted) - (n + 1) * np.sum(user_sorted)) / (n * np.sum(user_sorted))\n",
    "ax1.text(0.1, 0.85, f'Gini: {gini_user:.3f}', transform=ax1.transAxes, fontsize=12,\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Item Lorenz curve\n",
    "ax2 = axes[1]\n",
    "item_sorted = np.sort(item_activity.values)\n",
    "item_cum = np.cumsum(item_sorted) / item_sorted.sum()\n",
    "item_pct = np.arange(1, len(item_sorted) + 1) / len(item_sorted)\n",
    "ax2.plot(item_pct, item_cum, 'r-', linewidth=2, label='Actual')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', label='Perfect equality')\n",
    "ax2.fill_between(item_pct, item_cum, item_pct, alpha=0.2, color='red')\n",
    "ax2.set_xlabel('Cumulative % of Items')\n",
    "ax2.set_ylabel('Cumulative % of Ratings')\n",
    "ax2.set_title('Item Activity Lorenz Curve (Popularity)')\n",
    "ax2.legend()\n",
    "\n",
    "# Calculate Gini coefficient for items\n",
    "n = len(item_sorted)\n",
    "gini_item = (2 * np.sum(np.arange(1, n+1) * item_sorted) - (n + 1) * np.sum(item_sorted)) / (n * np.sum(item_sorted))\n",
    "ax2.text(0.1, 0.85, f'Gini: {gini_item:.3f}', transform=ax2.transAxes, fontsize=12,\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/eda_lorenz_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Gini coefficient for user activity: {gini_user:.3f}\")\n",
    "print(f\"Gini coefficient for item popularity: {gini_item:.3f}\")\n",
    "print(\"\\n(Gini = 0: perfect equality, Gini = 1: maximum inequality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Dynamics\n",
    "\n",
    "The Book Crossing dataset was collected in 2004, and while it doesn't have explicit timestamps, we can analyze temporal patterns through publication years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze publication years\n",
    "# First, merge ratings with books to get publication years\n",
    "ratings_with_year = ratings.merge(books[['isbn', 'year', 'title', 'author']], on='isbn', how='left')\n",
    "\n",
    "# Clean publication years (filter out clearly invalid values)\n",
    "valid_years = ratings_with_year[\n",
    "    (ratings_with_year['year'] >= 1800) & \n",
    "    (ratings_with_year['year'] <= 2004)  # Dataset was collected in 2004\n",
    "]['year']\n",
    "\n",
    "print(f\"Ratings with valid publication years: {len(valid_years):,} ({100*len(valid_years)/len(ratings):.1f}%)\")\n",
    "print(f\"\\nPublication year statistics:\")\n",
    "print(valid_years.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication year distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Full history\n",
    "ax1 = axes[0]\n",
    "year_counts = valid_years.value_counts().sort_index()\n",
    "ax1.bar(year_counts.index, year_counts.values, color='steelblue', alpha=0.7)\n",
    "ax1.set_xlabel('Publication Year')\n",
    "ax1.set_ylabel('Number of Ratings')\n",
    "ax1.set_title('Ratings by Book Publication Year (All Time)')\n",
    "\n",
    "# Recent years (1980-2004)\n",
    "ax2 = axes[1]\n",
    "recent_years = valid_years[valid_years >= 1980]\n",
    "recent_counts = recent_years.value_counts().sort_index()\n",
    "ax2.bar(recent_counts.index, recent_counts.values, color='coral', alpha=0.7)\n",
    "ax2.set_xlabel('Publication Year')\n",
    "ax2.set_ylabel('Number of Ratings')\n",
    "ax2.set_title('Ratings by Book Publication Year (1980-2004)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/eda_temporal_dynamics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze recency bias\n",
    "# What percentage of ratings are for books published in the last 5 years (1999-2004)?\n",
    "recent_5y = valid_years[valid_years >= 1999].count()\n",
    "recent_10y = valid_years[valid_years >= 1994].count()\n",
    "\n",
    "print(\"RECENCY BIAS ANALYSIS\")\n",
    "print(f\"\\nRatings for books published 1999-2004: {recent_5y:,} ({100*recent_5y/len(valid_years):.1f}%)\")\n",
    "print(f\"Ratings for books published 1994-2004: {recent_10y:,} ({100*recent_10y/len(valid_years):.1f}%)\")\n",
    "print(f\"Ratings for books published before 1994: {len(valid_years) - recent_10y:,} ({100*(len(valid_years) - recent_10y)/len(valid_years):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze rating patterns by publication decade\n",
    "ratings_with_year['decade'] = (ratings_with_year['year'] // 10 * 10).astype('Int64')\n",
    "\n",
    "decade_stats = ratings_with_year[\n",
    "    (ratings_with_year['decade'] >= 1950) & \n",
    "    (ratings_with_year['decade'] <= 2000) &\n",
    "    (ratings_with_year['rating'] > 0)  # Only explicit ratings\n",
    "].groupby('decade').agg({\n",
    "    'rating': ['count', 'mean', 'std'],\n",
    "    'isbn': 'nunique',\n",
    "    'user_id': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "decade_stats.columns = ['n_ratings', 'avg_rating', 'std_rating', 'n_books', 'n_users']\n",
    "print(\"Rating statistics by decade (explicit ratings only):\")\n",
    "print(decade_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Pathologies\n",
    "\n",
    "### 5.1 Popularity Skew (Long-Tail Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popularity analysis\n",
    "item_popularity = item_activity.sort_values(ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Calculate concentration metrics\n",
    "top_1_pct = int(len(item_popularity) * 0.01)\n",
    "top_5_pct = int(len(item_popularity) * 0.05)\n",
    "top_10_pct = int(len(item_popularity) * 0.10)\n",
    "top_20_pct = int(len(item_popularity) * 0.20)\n",
    "\n",
    "total_ratings = item_popularity.sum()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POPULARITY SKEW ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTop 1% of items ({top_1_pct:,} books) account for {100*item_popularity[:top_1_pct].sum()/total_ratings:.1f}% of ratings\")\n",
    "print(f\"Top 5% of items ({top_5_pct:,} books) account for {100*item_popularity[:top_5_pct].sum()/total_ratings:.1f}% of ratings\")\n",
    "print(f\"Top 10% of items ({top_10_pct:,} books) account for {100*item_popularity[:top_10_pct].sum()/total_ratings:.1f}% of ratings\")\n",
    "print(f\"Top 20% of items ({top_20_pct:,} books) account for {100*item_popularity[:top_20_pct].sum()/total_ratings:.1f}% of ratings\")\n",
    "\n",
    "# Bottom half\n",
    "bottom_half = len(item_popularity) // 2\n",
    "print(f\"\\nBottom 50% of items ({bottom_half:,} books) account for {100*item_popularity[-bottom_half:].sum()/total_ratings:.1f}% of ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize popularity distribution (Rank vs Rating count)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear scale\n",
    "ax1 = axes[0]\n",
    "ax1.plot(range(1, len(item_popularity) + 1), item_popularity.values, 'b-', linewidth=0.5)\n",
    "ax1.set_xlabel('Item Rank')\n",
    "ax1.set_ylabel('Number of Ratings')\n",
    "ax1.set_title('Item Popularity by Rank (Linear Scale)')\n",
    "ax1.axhline(item_popularity.median(), color='red', linestyle='--', \n",
    "            label=f'Median: {item_popularity.median():.0f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Log-log scale (Zipf's law check)\n",
    "ax2 = axes[1]\n",
    "ax2.loglog(range(1, len(item_popularity) + 1), item_popularity.values, 'b.', markersize=1)\n",
    "ax2.set_xlabel('Item Rank (log)')\n",
    "ax2.set_ylabel('Number of Ratings (log)')\n",
    "ax2.set_title('Item Popularity by Rank (Log-Log Scale)')\n",
    "\n",
    "# Fit power law to top items\n",
    "top_n = 10000\n",
    "ranks = np.arange(1, top_n + 1)\n",
    "ratings_top = item_popularity.values[:top_n]\n",
    "log_ranks = np.log(ranks)\n",
    "log_ratings = np.log(ratings_top)\n",
    "slope, intercept = np.polyfit(log_ranks, log_ratings, 1)\n",
    "ax2.plot(ranks, np.exp(intercept) * ranks ** slope, 'r-', linewidth=2, \n",
    "         label=f'Power law fit: α = {-slope:.2f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/eda_popularity_skew.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPower law exponent (Zipf's α): {-slope:.2f}\")\n",
    "print(\"(α ≈ 1 indicates classic Zipf's law; α > 1 indicates stronger concentration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular items\n",
    "top_books = ratings.groupby('isbn').agg({\n",
    "    'user_id': 'count',\n",
    "    'rating': 'mean'\n",
    "}).rename(columns={'user_id': 'n_ratings', 'rating': 'avg_rating'})\n",
    "top_books = top_books.merge(books[['isbn', 'title', 'author']], on='isbn', how='left')\n",
    "top_books = top_books.sort_values('n_ratings', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Rated Books:\")\n",
    "print(top_books.head(20)[['title', 'author', 'n_ratings', 'avg_rating']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Cold-Start Problem Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cold-start analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"COLD-START PROBLEM ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# User cold-start\n",
    "user_thresholds = [1, 2, 3, 5, 10, 20]\n",
    "print(\"\\nUSER COLD-START:\")\n",
    "for thresh in user_thresholds:\n",
    "    cold_users = (user_activity <= thresh).sum()\n",
    "    print(f\"  Users with ≤{thresh:2d} ratings: {cold_users:,} ({100*cold_users/len(user_activity):.1f}%)\")\n",
    "\n",
    "# Item cold-start\n",
    "print(\"\\nITEM COLD-START:\")\n",
    "for thresh in user_thresholds:\n",
    "    cold_items = (item_activity <= thresh).sum()\n",
    "    print(f\"  Items with ≤{thresh:2d} ratings: {cold_items:,} ({100*cold_items/len(item_activity):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cold-start severity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# User cold-start cumulative\n",
    "ax1 = axes[0]\n",
    "thresholds = range(1, 51)\n",
    "user_cold_pct = [(user_activity <= t).sum() / len(user_activity) * 100 for t in thresholds]\n",
    "ax1.plot(thresholds, user_cold_pct, 'b-', linewidth=2, marker='o', markersize=3)\n",
    "ax1.axhline(50, color='red', linestyle='--', alpha=0.5)\n",
    "ax1.axhline(80, color='orange', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Rating Threshold')\n",
    "ax1.set_ylabel('% of Users')\n",
    "ax1.set_title('User Cold-Start: % of Users with ≤X Ratings')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Item cold-start cumulative\n",
    "ax2 = axes[1]\n",
    "item_cold_pct = [(item_activity <= t).sum() / len(item_activity) * 100 for t in thresholds]\n",
    "ax2.plot(thresholds, item_cold_pct, 'r-', linewidth=2, marker='o', markersize=3)\n",
    "ax2.axhline(50, color='red', linestyle='--', alpha=0.5)\n",
    "ax2.axhline(80, color='orange', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Rating Threshold')\n",
    "ax2.set_ylabel('% of Items')\n",
    "ax2.set_title('Item Cold-Start: % of Items with ≤X Ratings')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/eda_cold_start.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Additional Pathology: Implicit vs Explicit Feedback Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze users who only have implicit ratings\n",
    "user_explicit_count = ratings[ratings['rating'] > 0].groupby('user_id').size()\n",
    "users_with_explicit = set(user_explicit_count.index)\n",
    "all_users = set(ratings['user_id'].unique())\n",
    "users_implicit_only = all_users - users_with_explicit\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IMPLICIT VS EXPLICIT FEEDBACK ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal unique users: {len(all_users):,}\")\n",
    "print(f\"Users with at least one explicit rating: {len(users_with_explicit):,} ({100*len(users_with_explicit)/len(all_users):.1f}%)\")\n",
    "print(f\"Users with ONLY implicit ratings: {len(users_implicit_only):,} ({100*len(users_implicit_only)/len(all_users):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze items with only implicit ratings\n",
    "item_explicit_count = ratings[ratings['rating'] > 0].groupby('isbn').size()\n",
    "items_with_explicit = set(item_explicit_count.index)\n",
    "all_items = set(ratings['isbn'].unique())\n",
    "items_implicit_only = all_items - items_with_explicit\n",
    "\n",
    "print(f\"\\nTotal unique items in ratings: {len(all_items):,}\")\n",
    "print(f\"Items with at least one explicit rating: {len(items_with_explicit):,} ({100*len(items_with_explicit)/len(all_items):.1f}%)\")\n",
    "print(f\"Items with ONLY implicit ratings: {len(items_implicit_only):,} ({100*len(items_implicit_only)/len(all_items):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Rating Bias Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze rating bias per user (explicit ratings only)\n",
    "explicit_ratings_df = ratings[ratings['rating'] > 0]\n",
    "\n",
    "user_rating_stats = explicit_ratings_df.groupby('user_id')['rating'].agg(['mean', 'std', 'count'])\n",
    "user_rating_stats = user_rating_stats[user_rating_stats['count'] >= 5]  # Users with at least 5 ratings\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"USER RATING BIAS ANALYSIS (users with ≥5 explicit ratings)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNumber of users analyzed: {len(user_rating_stats):,}\")\n",
    "print(f\"\\nDistribution of user mean ratings:\")\n",
    "print(user_rating_stats['mean'].describe())\n",
    "\n",
    "# How many users rate consistently high or low?\n",
    "harsh_raters = (user_rating_stats['mean'] < 5).sum()\n",
    "lenient_raters = (user_rating_stats['mean'] > 8).sum()\n",
    "print(f\"\\n'Harsh' raters (mean < 5): {harsh_raters:,} ({100*harsh_raters/len(user_rating_stats):.1f}%)\")\n",
    "print(f\"'Lenient' raters (mean > 8): {lenient_raters:,} ({100*lenient_raters/len(user_rating_stats):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize user rating bias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution of user mean ratings\n",
    "ax1 = axes[0]\n",
    "ax1.hist(user_rating_stats['mean'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(user_rating_stats['mean'].mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {user_rating_stats[\"mean\"].mean():.2f}')\n",
    "ax1.axvline(user_rating_stats['mean'].median(), color='orange', linestyle='--', \n",
    "            label=f'Median: {user_rating_stats[\"mean\"].median():.2f}')\n",
    "ax1.set_xlabel('User Average Rating')\n",
    "ax1.set_ylabel('Number of Users')\n",
    "ax1.set_title('Distribution of User Average Ratings')\n",
    "ax1.legend()\n",
    "\n",
    "# Distribution of user rating std\n",
    "ax2 = axes[1]\n",
    "ax2.hist(user_rating_stats['std'].dropna(), bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(user_rating_stats['std'].mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {user_rating_stats[\"std\"].mean():.2f}')\n",
    "ax2.set_xlabel('User Rating Standard Deviation')\n",
    "ax2.set_ylabel('Number of Users')\n",
    "ax2.set_title('Distribution of User Rating Variance')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/eda_rating_bias.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. User Demographics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution\n",
    "valid_ages = users[(users['age'] > 5) & (users['age'] < 100)]['age']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(valid_ages, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.axvline(valid_ages.mean(), color='red', linestyle='--', label=f'Mean: {valid_ages.mean():.1f}')\n",
    "ax.axvline(valid_ages.median(), color='orange', linestyle='--', label=f'Median: {valid_ages.median():.1f}')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Number of Users')\n",
    "ax.set_title('User Age Distribution')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/eda_age_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Users with valid age: {len(valid_ages):,} ({100*len(valid_ages)/len(users):.1f}%)\")\n",
    "print(f\"Age statistics:\")\n",
    "print(valid_ages.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Location analysis (if available)\nif 'location' in users.columns and users['location'].notna().any():\n    def extract_country(location):\n        if pd.isna(location):\n            return 'Unknown'\n        parts = str(location).split(',')\n        if len(parts) >= 1:\n            return parts[-1].strip()\n        return 'Unknown'\n\n    users['country'] = users['location'].apply(extract_country)\n    country_counts = users['country'].value_counts().head(20)\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n    country_counts.plot(kind='bar', ax=ax, color='steelblue', edgecolor='black')\n    ax.set_xlabel('Country')\n    ax.set_ylabel('Number of Users')\n    ax.set_title('Top 20 Countries by Number of Users')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig('../experiments/eda_country_distribution.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n    print(\"Top 10 countries:\")\n    print(country_counts.head(10))\nelse:\n    print(\"Location data not available in this version of the dataset.\")\n    print(\"This analysis is skipped.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics and Modeling Implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dataframe\n",
    "summary_stats = {\n",
    "    'Metric': [\n",
    "        'Total Ratings',\n",
    "        'Unique Users',\n",
    "        'Unique Items (Books)',\n",
    "        'Sparsity (all interactions)',\n",
    "        'Sparsity (explicit only)',\n",
    "        'Implicit Ratings (%)',\n",
    "        'Explicit Ratings (%)',\n",
    "        'Avg Ratings per User',\n",
    "        'Avg Ratings per Item',\n",
    "        'Median Ratings per User',\n",
    "        'Median Ratings per Item',\n",
    "        'Gini (User Activity)',\n",
    "        'Gini (Item Popularity)',\n",
    "        'Cold Users (≤5 ratings)',\n",
    "        'Cold Items (≤5 ratings)',\n",
    "    ],\n",
    "    'Value': [\n",
    "        f'{n_interactions:,}',\n",
    "        f'{n_users:,}',\n",
    "        f'{n_items:,}',\n",
    "        f'{sparsity*100:.4f}%',\n",
    "        f'{sparsity_explicit*100:.4f}%',\n",
    "        f'{100*implicit_count/len(ratings):.1f}%',\n",
    "        f'{100*explicit_count/len(ratings):.1f}%',\n",
    "        f'{n_interactions/n_users:.2f}',\n",
    "        f'{n_interactions/n_items:.2f}',\n",
    "        f'{user_activity.median():.0f}',\n",
    "        f'{item_activity.median():.0f}',\n",
    "        f'{gini_user:.3f}',\n",
    "        f'{gini_item:.3f}',\n",
    "        f'{(user_activity <= 5).sum():,} ({100*(user_activity <= 5).sum()/len(user_activity):.1f}%)',\n",
    "        f'{(item_activity <= 5).sum():,} ({100*(item_activity <= 5).sum()/len(item_activity):.1f}%)',\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "print(\"=\" * 60)\n",
    "print(\"BOOK CROSSING DATASET - SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to CSV\n",
    "summary_df.to_csv('../experiments/eda_summary_stats.csv', index=False)\n",
    "print(\"\\nSummary statistics saved to experiments/eda_summary_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Modeling Implications & Recommendations\n",
    "\n",
    "Based on this EDA, here are the key implications for building recommender systems:\n",
    "\n",
    "### 8.1 Data Preprocessing Recommendations\n",
    "\n",
    "1. **Separate implicit and explicit feedback**: The dataset has ~62% implicit ratings (rating=0). Consider:\n",
    "   - Training separate models for implicit vs explicit feedback\n",
    "   - Using implicit feedback for candidate generation, explicit for ranking\n",
    "   - Converting to binary feedback (interacted/not interacted)\n",
    "\n",
    "2. **Apply filtering thresholds**: Given the extreme cold-start:\n",
    "   - Filter users with <5 ratings for reliable evaluation\n",
    "   - Filter items with <10 ratings for stable item representations\n",
    "   - Document the filtering strategy clearly\n",
    "\n",
    "3. **Handle rating normalization**: Strong user bias observed:\n",
    "   - Apply mean-centering per user\n",
    "   - Consider z-score normalization for users with sufficient ratings\n",
    "\n",
    "### 8.2 Algorithm Selection Implications\n",
    "\n",
    "1. **Matrix Factorization**: \n",
    "   - High sparsity (99.99%) makes standard SVD challenging\n",
    "   - Use regularized methods (ALS, SGD with regularization)\n",
    "   - Start with low rank (k=10-50) due to sparsity\n",
    "   - Consider implicit feedback variants (Hu et al., 2008)\n",
    "\n",
    "2. **Neighborhood-based methods**:\n",
    "   - Item-based CF may work better than user-based (fewer items than users)\n",
    "   - Use weighted similarity with shrinkage\n",
    "   - Consider minimum support thresholds\n",
    "\n",
    "3. **Popularity baseline is strong**: \n",
    "   - Given severe popularity skew, simple popularity-based recommendation will be hard to beat\n",
    "   - Use popularity as a strong baseline\n",
    "   - Consider popularity-aware evaluation\n",
    "\n",
    "### 8.3 Evaluation Protocol Recommendations\n",
    "\n",
    "1. **Train/test split strategy**:\n",
    "   - Random split may overfit to popular items\n",
    "   - Consider leave-one-out or temporal split\n",
    "   - Stratify by user activity level\n",
    "\n",
    "2. **Metrics selection**:\n",
    "   - Use ranking metrics (NDCG, MAP, Hit Rate) not just RMSE\n",
    "   - Report coverage and novelty alongside accuracy\n",
    "   - Consider beyond-accuracy metrics (diversity, serendipity)\n",
    "\n",
    "3. **Cold-start evaluation**:\n",
    "   - Report performance separately for cold vs warm users/items\n",
    "   - Use cross-validation to ensure cold-start scenarios in test set\n",
    "\n",
    "### 8.4 Data Pathologies to Address\n",
    "\n",
    "| Pathology | Severity | Mitigation Strategy |\n",
    "|-----------|----------|--------------------|\n",
    "| Extreme sparsity | Very High (99.99%) | Implicit feedback, low-rank models |\n",
    "| Popularity skew | High (Gini > 0.8) | Inverse propensity weighting, popularity debiasing |\n",
    "| User cold-start | High (62% have ≤5 ratings) | Content features, popularity fallback |\n",
    "| Item cold-start | Very High (85% have ≤5 ratings) | Book metadata, author similarity |\n",
    "| Implicit/explicit imbalance | Moderate (62%/38%) | Hybrid approach |\n",
    "| Rating bias | Moderate | User mean normalization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEDA Complete! All figures saved to experiments/ directory.\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"  - eda_rating_distribution.png\")\n",
    "print(\"  - eda_sparsity_comparison.png\")\n",
    "print(\"  - eda_activity_distributions.png\")\n",
    "print(\"  - eda_lorenz_curves.png\")\n",
    "print(\"  - eda_temporal_dynamics.png\")\n",
    "print(\"  - eda_popularity_skew.png\")\n",
    "print(\"  - eda_cold_start.png\")\n",
    "print(\"  - eda_rating_bias.png\")\n",
    "print(\"  - eda_age_distribution.png\")\n",
    "print(\"  - eda_country_distribution.png\")\n",
    "print(\"  - eda_summary_stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}