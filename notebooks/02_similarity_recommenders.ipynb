{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity-Based Recommenders: Experiments and Evaluation\n",
    "\n",
    "This notebook implements and compares two classical recommendation paradigms:\n",
    "1. **Content-Based Filtering (CBF)**: Recommends items similar to user's preferences based on item metadata\n",
    "2. **Item-Item Collaborative Filtering (CF)**: Recommends items similar to user's rated items based on co-rating patterns\n",
    "\n",
    "For each approach, we compare two similarity functions:\n",
    "- CBF: **Cosine** vs **Jaccard** similarity\n",
    "- CF: **Cosine** vs **Pearson** correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from src.data.loader import load_all_data\n",
    "from src.data.splitter import create_temporal_split, SplitConfig\n",
    "from src.models import ContentBasedRecommender, ItemItemCFRecommender\n",
    "from src.evaluation import (\n",
    "    EvaluationPipeline,\n",
    "    GlobalMeanBaseline,\n",
    "    UserMeanBaseline,\n",
    "    ItemMeanBaseline,\n",
    "    PopularityBaseline,\n",
    "    print_evaluation_results,\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Temporal Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "ratings, books, users = load_all_data('../data/raw')\n",
    "\n",
    "# Create temporal split\n",
    "config = SplitConfig(\n",
    "    train_max_year=1999,\n",
    "    val_max_year=2001,\n",
    "    min_user_interactions=5,\n",
    "    min_item_interactions=5,\n",
    "    explicit_only=True\n",
    ")\n",
    "\n",
    "train_df, val_df, test_df, split_info = create_temporal_split(ratings, books, \"books\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the cold-start situation\n",
    "train_items = set(train_df['isbn'].unique())\n",
    "test_items = set(test_df['isbn'].unique())\n",
    "overlap = train_items & test_items\n",
    "\n",
    "print(\"TEMPORAL SPLIT - COLD-START ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Train items: {len(train_items):,}\")\n",
    "print(f\"Test items: {len(test_items):,}\")\n",
    "print(f\"Overlap (items in both): {len(overlap)}\")\n",
    "print(f\"\\nTest items are books published 2002-2004.\")\n",
    "print(f\"Train items are books published ≤1999.\")\n",
    "print(f\"→ ZERO overlap: CF cannot represent test items.\")\n",
    "print(f\"→ CBF can represent test items via metadata (author, publisher, year).\")\n",
    "\n",
    "# Shared users\n",
    "train_users = set(train_df['user_id'].unique())\n",
    "test_users = set(test_df['user_id'].unique())\n",
    "shared_users = train_users & test_users\n",
    "print(f\"\\nShared users (in both train and test): {len(shared_users):,}\")\n",
    "print(f\"Test-only users: {len(test_users - train_users):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluation pipeline\n",
    "K_VALUES = [5, 10, 20]\n",
    "pipeline = EvaluationPipeline(k_values=K_VALUES, relevance_threshold=6.0)\n",
    "\n",
    "# Store all results\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Models\n",
    "\n",
    "We first establish baselines to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Mean Baseline\n",
    "global_mean = GlobalMeanBaseline()\n",
    "global_mean.fit(train_df)\n",
    "all_results['Global Mean'] = pipeline.evaluate_rating_prediction(global_mean, test_df)\n",
    "print(f\"Global Mean → RMSE: {all_results['Global Mean']['rmse']:.4f}, MAE: {all_results['Global Mean']['mae']:.4f}\")\n",
    "\n",
    "# User Mean Baseline\n",
    "user_mean = UserMeanBaseline()\n",
    "user_mean.fit(train_df)\n",
    "all_results['User Mean'] = pipeline.evaluate_rating_prediction(user_mean, test_df)\n",
    "print(f\"User Mean  → RMSE: {all_results['User Mean']['rmse']:.4f}, MAE: {all_results['User Mean']['mae']:.4f}\")\n",
    "\n",
    "# Item Mean Baseline\n",
    "item_mean = ItemMeanBaseline()\n",
    "item_mean.fit(train_df)\n",
    "all_results['Item Mean'] = pipeline.evaluate_rating_prediction(item_mean, test_df)\n",
    "print(f\"Item Mean  → RMSE: {all_results['Item Mean']['rmse']:.4f}, MAE: {all_results['Item Mean']['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Content-Based Filtering\n",
    "\n",
    "### Item Representation\n",
    "\n",
    "Each book is represented as a sparse feature vector combining:\n",
    "- **Author TF-IDF**: Captures authorship similarity. TF-IDF weighting ensures rare authors provide stronger signal (sharing an uncommon author is more informative than sharing a prolific one).\n",
    "- **Publisher TF-IDF**: Captures publishing house affinity. Books from the same publisher often share genre/audience.\n",
    "- **Normalized Year**: Captures temporal proximity. Books from the same era are more likely to share themes.\n",
    "\n",
    "### Similarity Functions\n",
    "\n",
    "We compare two similarity functions:\n",
    "\n",
    "| Function | Formula | Properties |\n",
    "|----------|---------|------------|\n",
    "| **Cosine** | cos(A,B) = A·B / (\\|A\\|·\\|B\\|) | Scale-invariant, standard for TF-IDF, captures soft similarity |\n",
    "| **Jaccard** | J(A,B) = \\|A∩B\\| / \\|A∪B\\| | Set-based, binary feature overlap, more conservative matching |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Content-Based with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Based: Cosine Similarity\n",
    "print(\"Training Content-Based Recommender (Cosine)...\")\n",
    "t0 = time.time()\n",
    "\n",
    "cbf_cosine = ContentBasedRecommender(\n",
    "    books_df=books,\n",
    "    similarity_metric='cosine',\n",
    "    n_neighbors=50,\n",
    "    relevance_threshold=6.0,\n",
    ")\n",
    "cbf_cosine.fit(train_df)\n",
    "\n",
    "fit_time = time.time() - t0\n",
    "print(f\"Fit time: {fit_time:.1f}s\")\n",
    "print(f\"Item features shape: {cbf_cosine.item_features.shape}\")\n",
    "print(f\"Users with preference vectors: {len(cbf_cosine.user_pref_vectors):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CBF Cosine - Rating Prediction\n",
    "print(\"Evaluating rating prediction...\")\n",
    "t0 = time.time()\n",
    "cbf_cosine_rating = pipeline.evaluate_rating_prediction(cbf_cosine, test_df)\n",
    "print(f\"  RMSE: {cbf_cosine_rating['rmse']:.4f}\")\n",
    "print(f\"  MAE:  {cbf_cosine_rating['mae']:.4f}\")\n",
    "print(f\"  Time: {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CBF Cosine - Ranking\n",
    "print(\"Evaluating ranking (this may take a few minutes)...\")\n",
    "t0 = time.time()\n",
    "cbf_cosine_ranking = pipeline.evaluate_ranking(cbf_cosine, test_df, train_df, n_recommendations=20)\n",
    "print(f\"  NDCG@10:     {cbf_cosine_ranking.get('ndcg@10', 0):.4f}\")\n",
    "print(f\"  Precision@10: {cbf_cosine_ranking.get('precision@10', 0):.4f}\")\n",
    "print(f\"  Hit Rate@10:  {cbf_cosine_ranking.get('hit_rate@10', 0):.4f}\")\n",
    "print(f\"  Users eval'd: {cbf_cosine_ranking.get('users_evaluated', 0)}\")\n",
    "print(f\"  Time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Merge results\n",
    "all_results['CBF Cosine'] = {**cbf_cosine_rating, **cbf_cosine_ranking}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Content-Based with Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Based: Jaccard Similarity\n",
    "print(\"Training Content-Based Recommender (Jaccard)...\")\n",
    "t0 = time.time()\n",
    "\n",
    "cbf_jaccard = ContentBasedRecommender(\n",
    "    books_df=books,\n",
    "    similarity_metric='jaccard',\n",
    "    n_neighbors=50,\n",
    "    relevance_threshold=6.0,\n",
    ")\n",
    "cbf_jaccard.fit(train_df)\n",
    "\n",
    "print(f\"Fit time: {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CBF Jaccard - Rating Prediction\n",
    "print(\"Evaluating rating prediction...\")\n",
    "t0 = time.time()\n",
    "cbf_jaccard_rating = pipeline.evaluate_rating_prediction(cbf_jaccard, test_df)\n",
    "print(f\"  RMSE: {cbf_jaccard_rating['rmse']:.4f}\")\n",
    "print(f\"  MAE:  {cbf_jaccard_rating['mae']:.4f}\")\n",
    "print(f\"  Time: {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CBF Jaccard - Ranking\n",
    "print(\"Evaluating ranking (this may take a few minutes)...\")\n",
    "t0 = time.time()\n",
    "cbf_jaccard_ranking = pipeline.evaluate_ranking(cbf_jaccard, test_df, train_df, n_recommendations=20)\n",
    "print(f\"  NDCG@10:     {cbf_jaccard_ranking.get('ndcg@10', 0):.4f}\")\n",
    "print(f\"  Precision@10: {cbf_jaccard_ranking.get('precision@10', 0):.4f}\")\n",
    "print(f\"  Hit Rate@10:  {cbf_jaccard_ranking.get('hit_rate@10', 0):.4f}\")\n",
    "print(f\"  Users eval'd: {cbf_jaccard_ranking.get('users_evaluated', 0)}\")\n",
    "print(f\"  Time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "all_results['CBF Jaccard'] = {**cbf_jaccard_rating, **cbf_jaccard_ranking}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Item-Item Collaborative Filtering\n",
    "\n",
    "### Approach Justification\n",
    "\n",
    "We choose **item-item** over user-user CF because:\n",
    "1. **Stability**: Item similarity is more stable over time than user similarity (users acquire new interests; items don't change).\n",
    "2. **Scalability**: Comparable matrix dimensions in this dataset (~5.5K train items vs ~6.6K users).\n",
    "3. **Industry standard**: Amazon's original recommendation system used item-item CF.\n",
    "\n",
    "### Cold-Start Limitation\n",
    "\n",
    "With temporal split, **test items have zero training ratings**. This means:\n",
    "- `predict()` for test items falls back to user mean (cold-start)\n",
    "- `recommend()` only surfaces training items, so ranking metrics measure whether train items the user hasn't seen match their test preferences\n",
    "\n",
    "### Similarity Functions\n",
    "\n",
    "| Function | Formula | Properties |\n",
    "|----------|---------|------------|\n",
    "| **Cosine** | cos(i,j) = r_i · r_j / (\\|r_i\\|·\\|r_j\\|) | Standard CF baseline. Treats missing as zero. |\n",
    "| **Pearson** | Mean-centered cosine | Adjusts for item rating bias. Better for explicit ratings where items have different baseline popularity. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Item-Item CF with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-Item CF: Cosine Similarity\n",
    "t0 = time.time()\n",
    "\n",
    "cf_cosine = ItemItemCFRecommender(\n",
    "    similarity_metric='cosine',\n",
    "    n_neighbors=50,\n",
    ")\n",
    "cf_cosine.fit(train_df)\n",
    "\n",
    "print(f\"Fit time: {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CF Cosine - Rating Prediction\n",
    "print(\"Evaluating rating prediction...\")\n",
    "t0 = time.time()\n",
    "cf_cosine_rating = pipeline.evaluate_rating_prediction(cf_cosine, test_df)\n",
    "print(f\"  RMSE: {cf_cosine_rating['rmse']:.4f}\")\n",
    "print(f\"  MAE:  {cf_cosine_rating['mae']:.4f}\")\n",
    "print(f\"  Time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Report cold-start rate\n",
    "n_cold = sum(1 for isbn in test_df['isbn'] if isbn not in cf_cosine.isbn_to_idx)\n",
    "print(f\"\\n  Cold-start predictions: {n_cold:,}/{len(test_df):,} ({100*n_cold/len(test_df):.1f}%)\")\n",
    "print(f\"  → All test items are cold-start (published 2002-2004, not in training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CF Cosine - Ranking\n",
    "print(\"Evaluating ranking...\")\n",
    "t0 = time.time()\n",
    "cf_cosine_ranking = pipeline.evaluate_ranking(cf_cosine, test_df, train_df, n_recommendations=20)\n",
    "print(f\"  NDCG@10:     {cf_cosine_ranking.get('ndcg@10', 0):.4f}\")\n",
    "print(f\"  Hit Rate@10:  {cf_cosine_ranking.get('hit_rate@10', 0):.4f}\")\n",
    "print(f\"  Users eval'd: {cf_cosine_ranking.get('users_evaluated', 0)}\")\n",
    "print(f\"  Time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "all_results['CF Cosine'] = {**cf_cosine_rating, **cf_cosine_ranking}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Item-Item CF with Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-Item CF: Pearson Correlation\n",
    "t0 = time.time()\n",
    "\n",
    "cf_pearson = ItemItemCFRecommender(\n",
    "    similarity_metric='pearson',\n",
    "    n_neighbors=50,\n",
    ")\n",
    "cf_pearson.fit(train_df)\n",
    "\n",
    "print(f\"Fit time: {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CF Pearson - Rating Prediction\n",
    "print(\"Evaluating rating prediction...\")\n",
    "t0 = time.time()\n",
    "cf_pearson_rating = pipeline.evaluate_rating_prediction(cf_pearson, test_df)\n",
    "print(f\"  RMSE: {cf_pearson_rating['rmse']:.4f}\")\n",
    "print(f\"  MAE:  {cf_pearson_rating['mae']:.4f}\")\n",
    "print(f\"  Time: {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CF Pearson - Ranking\n",
    "print(\"Evaluating ranking...\")\n",
    "t0 = time.time()\n",
    "cf_pearson_ranking = pipeline.evaluate_ranking(cf_pearson, test_df, train_df, n_recommendations=20)\n",
    "print(f\"  NDCG@10:     {cf_pearson_ranking.get('ndcg@10', 0):.4f}\")\n",
    "print(f\"  Hit Rate@10:  {cf_pearson_ranking.get('hit_rate@10', 0):.4f}\")\n",
    "print(f\"  Users eval'd: {cf_pearson_ranking.get('users_evaluated', 0)}\")\n",
    "print(f\"  Time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "all_results['CF Pearson'] = {**cf_pearson_rating, **cf_pearson_ranking}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison table\n",
    "metrics_to_show = ['rmse', 'mae', 'ndcg@10', 'precision@10', 'recall@10', 'hit_rate@10', 'catalog_coverage@20']\n",
    "model_order = ['Global Mean', 'User Mean', 'Item Mean', \n",
    "               'CBF Cosine', 'CBF Jaccard', \n",
    "               'CF Cosine', 'CF Pearson']\n",
    "\n",
    "comparison = pd.DataFrame(index=model_order, columns=metrics_to_show)\n",
    "\n",
    "for model_name in model_order:\n",
    "    if model_name in all_results:\n",
    "        for metric in metrics_to_show:\n",
    "            val = all_results[model_name].get(metric, None)\n",
    "            if val is not None:\n",
    "                comparison.loc[model_name, metric] = f\"{val:.4f}\"\n",
    "            else:\n",
    "                comparison.loc[model_name, metric] = \"N/A\"\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\" * 90)\n",
    "print(comparison.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rating prediction metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "models_with_rmse = [m for m in model_order if all_results.get(m, {}).get('rmse') is not None]\n",
    "rmse_vals = [all_results[m]['rmse'] for m in models_with_rmse]\n",
    "mae_vals = [all_results[m]['mae'] for m in models_with_rmse]\n",
    "\n",
    "colors = ['#95a5a6'] * 3 + ['#2ecc71'] * 2 + ['#3498db'] * 2\n",
    "colors = colors[:len(models_with_rmse)]\n",
    "\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(range(len(models_with_rmse)), rmse_vals, color=colors, edgecolor='black')\n",
    "ax1.set_xticks(range(len(models_with_rmse)))\n",
    "ax1.set_xticklabels(models_with_rmse, rotation=45, ha='right')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.set_title('Rating Prediction: RMSE (lower is better)')\n",
    "for bar, val in zip(bars, rmse_vals):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax2 = axes[1]\n",
    "bars = ax2.bar(range(len(models_with_rmse)), mae_vals, color=colors, edgecolor='black')\n",
    "ax2.set_xticks(range(len(models_with_rmse)))\n",
    "ax2.set_xticklabels(models_with_rmse, rotation=45, ha='right')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.set_title('Rating Prediction: MAE (lower is better)')\n",
    "for bar, val in zip(bars, mae_vals):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/similarity_rating_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ranking metrics (only models that have ranking results)\n",
    "ranking_models = [m for m in model_order if all_results.get(m, {}).get('ndcg@10') is not None]\n",
    "\n",
    "if ranking_models:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    ranking_colors = []\n",
    "    for m in ranking_models:\n",
    "        if 'CBF' in m:\n",
    "            ranking_colors.append('#2ecc71')\n",
    "        elif 'CF' in m:\n",
    "            ranking_colors.append('#3498db')\n",
    "        else:\n",
    "            ranking_colors.append('#95a5a6')\n",
    "    \n",
    "    for idx, (metric, title) in enumerate([\n",
    "        ('ndcg@10', 'NDCG@10'),\n",
    "        ('precision@10', 'Precision@10'),\n",
    "        ('hit_rate@10', 'Hit Rate@10')\n",
    "    ]):\n",
    "        ax = axes[idx]\n",
    "        vals = [all_results[m].get(metric, 0) for m in ranking_models]\n",
    "        bars = ax.bar(range(len(ranking_models)), vals, color=ranking_colors, edgecolor='black')\n",
    "        ax.set_xticks(range(len(ranking_models)))\n",
    "        ax.set_xticklabels(ranking_models, rotation=45, ha='right')\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_title(f'{title} (higher is better)')\n",
    "        for bar, val in zip(bars, vals):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                    f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../experiments/similarity_ranking_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No ranking results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example recommendations for a few users\n",
    "sample_users = list(set(train_df['user_id'].unique()) & set(test_df['user_id'].unique()))[:3]\n",
    "\n",
    "for user_id in sample_users:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"User: {user_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # User's training history\n",
    "    user_train = train_df[train_df['user_id'] == user_id].merge(\n",
    "        books[['isbn', 'title', 'author']], on='isbn', how='left'\n",
    "    ).sort_values('rating', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTraining history (top 5 by rating):\")\n",
    "    for _, row in user_train.head(5).iterrows():\n",
    "        title = str(row.get('title', 'Unknown'))[:50]\n",
    "        author = str(row.get('author', 'Unknown'))[:30]\n",
    "        print(f\"  [{row['rating']:.0f}] {title} — {author}\")\n",
    "    \n",
    "    # Test items (what they actually rated)\n",
    "    user_test = test_df[test_df['user_id'] == user_id].merge(\n",
    "        books[['isbn', 'title', 'author']], on='isbn', how='left'\n",
    "    ).sort_values('rating', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTest items (rated ≥6 = relevant):\")\n",
    "    for _, row in user_test.head(5).iterrows():\n",
    "        relevant = '✓' if row['rating'] >= 6 else ' '\n",
    "        title = str(row.get('title', 'Unknown'))[:50]\n",
    "        print(f\"  [{row['rating']:.0f}]{relevant} {title}\")\n",
    "    \n",
    "    # CBF Cosine recommendations\n",
    "    exclude = set(cbf_cosine.user_profiles.get(user_id, {}).keys())\n",
    "    cbf_recs = cbf_cosine.recommend(user_id, 5, exclude)\n",
    "    \n",
    "    print(f\"\\nCBF Cosine Top-5 Recommendations:\")\n",
    "    for isbn in cbf_recs:\n",
    "        book_info = books[books['isbn'] == isbn]\n",
    "        if len(book_info) > 0:\n",
    "            title = str(book_info.iloc[0].get('title', 'Unknown'))[:50]\n",
    "            author = str(book_info.iloc[0].get('author', 'Unknown'))[:30]\n",
    "            print(f\"  → {title} — {author}\")\n",
    "        else:\n",
    "            print(f\"  → {isbn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis and Discussion\n",
    "\n",
    "### 7.1 Content-Based Filtering: Cosine vs Jaccard\n",
    "\n",
    "**Cosine similarity** operates on continuous TF-IDF weights, capturing soft similarity: two books sharing a relatively rare author receive high similarity even if their publisher or year differ. The TF-IDF weighting naturally emphasizes distinctive features (rare authors/publishers).\n",
    "\n",
    "**Jaccard similarity** operates on binary feature presence (does this feature exist or not?), ignoring the TF-IDF magnitudes. This makes it more conservative: two books must share the exact same feature tokens to be considered similar. It cannot distinguish between a rare author match (strong signal) and a common author match (weak signal).\n",
    "\n",
    "**Expected behavior**: Cosine should outperform Jaccard because TF-IDF weighting provides richer similarity signal than binary feature overlap.\n",
    "\n",
    "### 7.2 Collaborative Filtering: Cosine vs Pearson\n",
    "\n",
    "**Cosine similarity** on raw rating vectors treats missing ratings as zeros. With 99.99% sparsity, this means similarity is dominated by the few items that users co-rated.\n",
    "\n",
    "**Pearson correlation** (adjusted cosine) centers each item's ratings by its mean before computing cosine. This removes item popularity bias: a book with average rating 9 and a book with average rating 5 are compared on their deviation patterns, not absolute values. This is theoretically preferred for explicit rating data.\n",
    "\n",
    "**Expected behavior**: Pearson should produce better rating predictions than cosine for known items, since the Book Crossing dataset exhibits strong item rating bias (EDA found ratings concentrated around 7-8).\n",
    "\n",
    "### 7.3 CBF vs CF: The Cold-Start Story\n",
    "\n",
    "The temporal evaluation split creates a complete cold-start scenario for CF:\n",
    "- **100% of test items** are unseen in training (books published 2002-2004 vs ≤1999)\n",
    "- CF's `predict()` always falls back to user mean for test items\n",
    "- CF's `recommend()` only surfaces training items, which never overlap with test relevant items\n",
    "\n",
    "This is not a failure of the CF implementation — it is a **fundamental limitation** of collaborative filtering when facing new items. Content-based filtering handles this by using metadata features (author, publisher, year) to represent items regardless of rating history.\n",
    "\n",
    "### 7.4 Key Takeaways\n",
    "\n",
    "1. **Content-based filtering is essential** for cold-start scenarios. When new items have no interaction history, metadata features are the only available signal.\n",
    "\n",
    "2. **Cosine similarity is generally preferred** for TF-IDF features (CBF) because it leverages continuous weights. Jaccard is more appropriate when features are inherently binary.\n",
    "\n",
    "3. **Pearson correlation is theoretically superior** for explicit rating CF because it adjusts for item bias, but in practice both CF variants are hampered by the cold-start problem under temporal evaluation.\n",
    "\n",
    "4. **Hybrid approaches** combining CF and CBF would address both scenarios: use CF for known items and CBF for new items. This motivates the next phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "results_df.to_csv('../experiments/similarity_results.csv')\n",
    "print(\"Results saved to experiments/similarity_results.csv\")\n",
    "print(\"\\nFigures saved:\")\n",
    "print(\"  - experiments/similarity_rating_comparison.png\")\n",
    "print(\"  - experiments/similarity_ranking_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
